{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import Required Libraries"
      ],
      "metadata": {
        "id": "dZiSYIePog6L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0jl8oAOAQthq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qegEWDISncHV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "from collections import Counter\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, Add\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from pycocoevalcap.cider.cider import Cider\n",
        "from tensorflow.keras.layers import RepeatVector\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "import cv2\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = VGG16(weights='imagenet', include_top=False)  # Exclude the dense layers"
      ],
      "metadata": {
        "id": "Be8AiMwxnfZI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_image(image_path):\n",
        "    image = load_img(image_path, target_size=(224, 224))  # Resize image\n",
        "    image = img_to_array(image)  # Convert to array\n",
        "    image = preprocess_input(image)  # Apply VGG16 preprocessing\n",
        "    image = np.expand_dims(image, axis=0)  # Add batch dimension\n",
        "    return image"
      ],
      "metadata": {
        "id": "QKNgnmDqoY4W"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(image_dir):\n",
        "    features = {}\n",
        "    for img_name in tqdm(os.listdir(image_dir)):\n",
        "        img_path = os.path.join(image_dir, img_name)\n",
        "        if img_name.lower().endswith(('png', 'jpg', 'jpeg')):\n",
        "            image = preprocess_image(img_path)\n",
        "            feature = model.predict(image, verbose=0)\n",
        "            features[img_name] = feature.flatten()\n",
        "    return features"
      ],
      "metadata": {
        "id": "XnNFc4IkorQ2"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_dir = '/content/Images/Images'  # Replace with actual path\n",
        "features = extract_features(image_dir)\n",
        "# Save features as a numpy file\n",
        "np.save('/content/image_features.npy', features)\n",
        "print(\"Features saved successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScS6m23uouC3",
        "outputId": "4c26e745-8547-4a8d-855c-99d1c015546a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 8091/8091 [10:21<00:00, 13.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features = np.load('/content/image_features.npy', allow_pickle=True).item()"
      ],
      "metadata": {
        "id": "vb8Y2YzaUPDc"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_caption(caption):\n",
        "    caption = caption.lower()  # Convert to lowercase\n",
        "    caption = re.sub(r'[^a-z\\s]', '', caption)  # Remove special characters\n",
        "    caption = re.sub(r'\\s+', ' ', caption).strip()  # Remove extra spaces\n",
        "    return caption"
      ],
      "metadata": {
        "id": "CPQ9-EI6Ww5J"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_clean_captions(captions_file):\n",
        "    captions_dict = {}\n",
        "    with open(captions_file, 'r') as file:\n",
        "        next(file)  # Skip the header line\n",
        "        for line in file:\n",
        "            img_name, caption = line.strip().split(',', 1)\n",
        "            clean_cap = clean_caption(caption)\n",
        "            if img_name not in captions_dict:\n",
        "                captions_dict[img_name] = []\n",
        "            captions_dict[img_name].append(clean_cap)\n",
        "    return captions_dict"
      ],
      "metadata": {
        "id": "gqEh95uPWzx9"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "captions_file = '/content/drive/MyDrive/DL Lab 6&7/captions.txt'\n",
        "captions = load_and_clean_captions(captions_file)"
      ],
      "metadata": {
        "id": "FkYq2GHcW2W5"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vocabulary(captions_dict, threshold=5):\n",
        "    all_words = []\n",
        "    for caption_list in captions_dict.values():\n",
        "        for caption in caption_list:\n",
        "            all_words.extend(caption.split())\n",
        "    word_counts = Counter(all_words)\n",
        "    # Filter words by threshold\n",
        "    vocab = {word for word, count in word_counts.items() if count >= threshold}\n",
        "    return vocab"
      ],
      "metadata": {
        "id": "LcBVaN2rXA8l"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = build_vocabulary(captions, threshold=5)\n",
        "print(f\"Vocabulary size: {len(vocab)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnB-_zW2XDQE",
        "outputId": "5ddb21da-5c31-439e-e2b6-daaffdce7ad4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 2984\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_mappings(vocab):\n",
        "    vocab = sorted(vocab)  # Sort for consistent indexing\n",
        "    word2idx = {word: idx+1 for idx, word in enumerate(vocab)}\n",
        "    word2idx['<start>'] = len(word2idx) + 1\n",
        "    word2idx['<end>'] = len(word2idx) + 1\n",
        "    idx2word = {idx: word for word, idx in word2idx.items()}\n",
        "    return word2idx, idx2word"
      ],
      "metadata": {
        "id": "fK492WFAXFCp"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2idx, idx2word = create_mappings(vocab)"
      ],
      "metadata": {
        "id": "VgbXOMEzXHG1"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def captions_to_sequences(captions_dict, word2idx):\n",
        "    sequences = {}\n",
        "    for img_name, caption_list in captions_dict.items():\n",
        "        sequences[img_name] = []\n",
        "        for caption in caption_list:\n",
        "            seq = [word2idx['<start>']]\n",
        "            seq.extend([word2idx[word] for word in caption.split() if word in word2idx])\n",
        "            seq.append(word2idx['<end>'])\n",
        "            sequences[img_name].append(seq)\n",
        "    return sequences"
      ],
      "metadata": {
        "id": "3Ziy0i_JXJCd"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = captions_to_sequences(captions, word2idx)"
      ],
      "metadata": {
        "id": "a2OqFEOlXKrx"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into train, and test\n",
        "image_names = list(features.keys())\n",
        "train_imgs, test_imgs = train_test_split(image_names, test_size=0.15, random_state=42)\n",
        "\n",
        "print(f\"Train: {len(train_imgs)}, Test: {len(test_imgs)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wwpc8cVShaLG",
        "outputId": "d0cb91d0-1800-4146-e2d0-f221b4cfbf6c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 6877, Test: 1214\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_features = {img: features[img] for img in train_imgs if img in features}\n",
        "test_features = {img: features[img] for img in test_imgs if img in features}\n",
        "\n",
        "train_captions = {img: sequences[img] for img in train_imgs if img in sequences}\n",
        "test_captions = {img: sequences[img] for img in test_imgs if img in sequences}"
      ],
      "metadata": {
        "id": "CVEzYEjXhgj4"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_caption_model(vocab_size, max_caption_length, embedding_dim, feature_vector_dim):\n",
        "    # Image feature vector input\n",
        "    image_input = Input(shape=(feature_vector_dim,), name=\"image_input\")\n",
        "    image_dense = Dense(embedding_dim, activation='relu', name=\"image_dense\")(image_input)\n",
        "    image_dropout = Dropout(0.5, name=\"image_dropout\")(image_dense)  # Dropout added here\n",
        "\n",
        "    # Repeat image features to match the sequence length of text input\n",
        "    image_repeated = RepeatVector(max_caption_length, name=\"image_repeat\")(image_dropout)\n",
        "\n",
        "    # Text input\n",
        "    text_input = Input(shape=(max_caption_length,), name=\"text_input\")\n",
        "    text_embedding = Embedding(vocab_size, embedding_dim, mask_zero=True, name=\"text_embedding\")(text_input)\n",
        "    text_lstm = LSTM(256, return_sequences=True, dropout=0.5, name=\"text_lstm\")(text_embedding)  # Dropout in LSTM\n",
        "\n",
        "    # Combine image and text features\n",
        "    combined = Add(name=\"add_features\")([image_repeated, text_lstm])\n",
        "    combined_lstm = LSTM(256, return_sequences=False, dropout=0.5, name=\"combined_lstm\")(combined)  # Dropout in LSTM\n",
        "\n",
        "    # Dense output layer to predict the next word\n",
        "    output = Dense(vocab_size, activation='softmax', name=\"output\")(combined_lstm)\n",
        "\n",
        "    # Define the model\n",
        "    model = Model(inputs=[image_input, text_input], outputs=output, name=\"caption_generator\")\n",
        "    return model"
      ],
      "metadata": {
        "id": "ziAtIIpcXMV0"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(word2idx) + 1  # Add 1 for padding\n",
        "embedding_dim = 256\n",
        "feature_vector_dim = 25088  # Update to the flattened size\n",
        "max_caption_length = 20  # Define based on your dataset\n",
        "\n",
        "model = build_caption_model(vocab_size, max_caption_length, embedding_dim, feature_vector_dim)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "cVRKWLwzXODQ",
        "outputId": "5a1162c4-350c-48be-cbe9-43ec261aad98"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"caption_generator\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"caption_generator\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ image_input (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25088\u001b[0m)          │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ image_dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │      \u001b[38;5;34m6,422,784\u001b[0m │ image_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ text_input (\u001b[38;5;33mInputLayer\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ image_dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ image_dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ text_embedding            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │        \u001b[38;5;34m764,672\u001b[0m │ text_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)               │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ not_equal (\u001b[38;5;33mNotEqual\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ text_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ image_repeat              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ image_dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mRepeatVector\u001b[0m)            │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ text_lstm (\u001b[38;5;33mLSTM\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │        \u001b[38;5;34m525,312\u001b[0m │ text_embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
              "│                           │                        │                │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_features (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ image_repeat[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
              "│                           │                        │                │ text_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ combined_lstm (\u001b[38;5;33mLSTM\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m525,312\u001b[0m │ add_features[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ output (\u001b[38;5;33mDense\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2987\u001b[0m)           │        \u001b[38;5;34m767,659\u001b[0m │ combined_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ image_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25088</span>)          │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ image_dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │      <span style=\"color: #00af00; text-decoration-color: #00af00\">6,422,784</span> │ image_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ text_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ image_dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ image_dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ text_embedding            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">764,672</span> │ text_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)               │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ not_equal (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ text_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ image_repeat              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ image_dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RepeatVector</span>)            │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ text_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │ text_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
              "│                           │                        │                │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_features (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ image_repeat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
              "│                           │                        │                │ text_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ combined_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │ add_features[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2987</span>)           │        <span style=\"color: #00af00; text-decoration-color: #00af00\">767,659</span> │ combined_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m9,005,739\u001b[0m (34.35 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,005,739</span> (34.35 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m9,005,739\u001b[0m (34.35 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,005,739</span> (34.35 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def data_generator_tf(features, sequences, max_caption_length, vocab_size):\n",
        "    def generator():\n",
        "        for img_name, caption_list in sequences.items():\n",
        "            if img_name not in features:\n",
        "                continue\n",
        "            for caption in caption_list:\n",
        "                for i in range(1, len(caption)):\n",
        "                    input_seq = caption[:i]\n",
        "                    target_word = caption[i]\n",
        "\n",
        "                    # Ensure sequence length doesn't exceed max_caption_length\n",
        "                    if len(input_seq) > max_caption_length:\n",
        "                        input_seq = input_seq[:max_caption_length]\n",
        "\n",
        "                    # Pad sequence to exactly max_caption_length\n",
        "                    input_seq = np.pad(\n",
        "                        input_seq,\n",
        "                        (0, max_caption_length - len(input_seq)),\n",
        "                        mode='constant'\n",
        "                    )\n",
        "\n",
        "                    # Yield data as tuples\n",
        "                    yield (features[img_name], input_seq), to_categorical(target_word, num_classes=vocab_size)\n",
        "\n",
        "    # Define output types and shapes\n",
        "    output_signature = (\n",
        "        (tf.TensorSpec(shape=(25088,), dtype=tf.float32),  # Feature vector shape\n",
        "         tf.TensorSpec(shape=(max_caption_length,), dtype=tf.int32)),  # Caption sequence shape\n",
        "        tf.TensorSpec(shape=(vocab_size,), dtype=tf.float32)  # Target word shape\n",
        "    )\n",
        "\n",
        "    return tf.data.Dataset.from_generator(generator, output_signature=output_signature)\n"
      ],
      "metadata": {
        "id": "OZ1nLw93XQQa"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,  # Stop if val_loss doesn't improve for 5 epochs\n",
        "    restore_best_weights=True\n",
        ")"
      ],
      "metadata": {
        "id": "5ymdGYYBXWui"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "# Total number of caption data points\n",
        "total_data_points = sum(len(caption) - 1 for caption_list in train_captions.values() for caption in caption_list)\n",
        "# Compute steps per epoch\n",
        "steps_per_epoch = total_data_points // batch_size\n",
        "validation_steps = steps_per_epoch // 10  # Use 10% of steps for validation\n",
        "\n",
        "# Create datasets for training and validation\n",
        "train_dataset = data_generator_tf(train_features, train_captions, max_caption_length, vocab_size).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "validation_dataset = data_generator_tf(train_features, train_captions, max_caption_length, vocab_size).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "checkpoint = ModelCheckpoint(\n",
        "    filepath='/content/checkpoints/best_model.keras',\n",
        "    save_best_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "model.fit(\n",
        "    train_dataset,\n",
        "    epochs=50,\n",
        "    steps_per_epoch=steps_per_epoch,  # Specify steps per epoch\n",
        "    validation_data=validation_dataset,\n",
        "    validation_steps=validation_steps,  # Specify validation steps\n",
        "    callbacks=[early_stopping, checkpoint]\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFt2aKgYXZc4",
        "outputId": "7568bb74-c822-4a98-f7bb-17853ab7fcc1"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m6204/6205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.1299 - loss: 5.5145\n",
            "Epoch 1: val_accuracy improved from -inf to 0.13627, saving model to /content/checkpoints/best_model.keras\n",
            "\u001b[1m6205/6205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m273s\u001b[0m 43ms/step - accuracy: 0.1299 - loss: 5.5145 - val_accuracy: 0.1363 - val_loss: 5.3414\n",
            "Epoch 2/50\n",
            "\u001b[1m   1/6205\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:32\u001b[0m 25ms/step - accuracy: 0.0938 - loss: 5.3668"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self.gen.throw(typ, value, traceback)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2: val_accuracy did not improve from 0.13627\n",
            "\u001b[1m6205/6205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - accuracy: 0.0938 - loss: 5.3668 - val_accuracy: 0.1342 - val_loss: 5.4053\n",
            "Epoch 3/50\n",
            "\u001b[1m6204/6205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.1334 - loss: 5.4553\n",
            "Epoch 3: val_accuracy improved from 0.13627 to 0.13647, saving model to /content/checkpoints/best_model.keras\n",
            "\u001b[1m6205/6205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 42ms/step - accuracy: 0.1334 - loss: 5.4553 - val_accuracy: 0.1365 - val_loss: 5.3770\n",
            "Epoch 4/50\n",
            "\u001b[1m   1/6205\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:33\u001b[0m 25ms/step - accuracy: 0.0938 - loss: 5.5471\n",
            "Epoch 4: val_accuracy did not improve from 0.13647\n",
            "\u001b[1m6205/6205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - accuracy: 0.0938 - loss: 5.5471 - val_accuracy: 0.1299 - val_loss: 5.4385\n",
            "Epoch 5/50\n",
            "\u001b[1m6204/6205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.1333 - loss: 5.4697\n",
            "Epoch 5: val_accuracy did not improve from 0.13647\n",
            "\u001b[1m6205/6205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m260s\u001b[0m 42ms/step - accuracy: 0.1333 - loss: 5.4697 - val_accuracy: 0.1323 - val_loss: 5.4013\n",
            "Epoch 6/50\n",
            "\u001b[1m   1/6205\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:25\u001b[0m 23ms/step - accuracy: 0.0938 - loss: 5.6024\n",
            "Epoch 6: val_accuracy did not improve from 0.13647\n",
            "\u001b[1m6205/6205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - accuracy: 0.0938 - loss: 5.6024 - val_accuracy: 0.1362 - val_loss: 5.3834\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7dbb15290250>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the best model after training\n",
        "best_model = load_model('/content/checkpoints/best_model.keras')"
      ],
      "metadata": {
        "id": "7tbGKuTApHT_"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, features, sequences, idx2word, max_caption_length):\n",
        "    smoothing_function = SmoothingFunction().method4\n",
        "    bleu_scores = []\n",
        "    cider_scorer = Cider()\n",
        "\n",
        "    references = []\n",
        "    hypotheses = []\n",
        "\n",
        "    for img_name, caption_list in sequences.items():\n",
        "        if img_name not in features:\n",
        "            continue\n",
        "\n",
        "        # Get the feature for the image\n",
        "        image_feature = features[img_name]\n",
        "        image_feature = np.expand_dims(image_feature, axis=0)  # Add batch dimension\n",
        "\n",
        "        # Generate a caption\n",
        "        generated_caption = []\n",
        "        input_seq = np.zeros((1, max_caption_length))  # Initialize input sequence\n",
        "        input_seq[0, 0] = word2idx['<start>']  # Start with the <start> token\n",
        "\n",
        "        for i in range(max_caption_length - 1):\n",
        "            predictions = model.predict([image_feature, input_seq], verbose=0)\n",
        "            next_word_idx = np.argmax(predictions[0])  # Get index of the predicted word\n",
        "            if next_word_idx == word2idx['<end>']:\n",
        "                break\n",
        "            generated_caption.append(idx2word[next_word_idx])\n",
        "            input_seq[0, i + 1] = next_word_idx  # Update input sequence with predicted word\n",
        "\n",
        "        # Join the predicted words to form a sentence\n",
        "        generated_sentence = ' '.join(generated_caption)\n",
        "\n",
        "        # Convert integer sequences in caption_list to words\n",
        "        references.append([[idx2word[idx] for idx in caption if idx in idx2word] for caption in caption_list])\n",
        "        hypotheses.append(generated_sentence.split())\n",
        "\n",
        "        # Compute BLEU score for the generated sentence\n",
        "        bleu_score = sentence_bleu(references[-1], hypotheses[-1], smoothing_function=smoothing_function)\n",
        "        bleu_scores.append(bleu_score)\n",
        "\n",
        "    # Compute CIDEr score\n",
        "    cider_score, _ = cider_scorer.compute_score(references, hypotheses)\n",
        "\n",
        "    print(f\"Average BLEU Score: {np.mean(bleu_scores):.4f}\")\n",
        "    print(f\"CIDEr Score: {cider_score:.4f}\")\n",
        "    return np.mean(bleu_scores), cider_score"
      ],
      "metadata": {
        "id": "g-a6bIWYdjxz"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "average_bleu, cider = evaluate_model(model, test_features, test_captions, idx2word, max_caption_length)"
      ],
      "metadata": {
        "id": "waWCyUnxXbe1",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_caption(image, model, idx2word, word2idx, max_caption_length):\n",
        "    \"\"\"Generate a caption for an input image.\"\"\"\n",
        "    feature = model.predict(image)  # Extract image features\n",
        "    feature = feature.flatten()\n",
        "    feature = np.expand_dims(feature, axis=0)\n",
        "\n",
        "    # Start generating the caption\n",
        "    caption = []\n",
        "    input_seq = np.zeros((1, max_caption_length))\n",
        "    input_seq[0, 0] = word2idx['<start>']\n",
        "\n",
        "    for i in range(max_caption_length - 1):\n",
        "        predictions = model.predict([feature, input_seq], verbose=0)\n",
        "        next_word_idx = np.argmax(predictions[0])\n",
        "        if next_word_idx == word2idx['<end>']:\n",
        "            break\n",
        "        caption.append(idx2word[next_word_idx])\n",
        "        input_seq[0, i + 1] = next_word_idx\n",
        "\n",
        "    return ' '.join(caption)\n",
        "\n",
        "# Start camera capture\n",
        "cap = cv2.VideoCapture(0)  # Use 0 for the default webcam\n",
        "if not cap.isOpened():\n",
        "    print(\"Cannot open camera\")\n",
        "    exit()\n",
        "\n",
        "print(\"Starting camera. Press 'q' to exit.\")\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()  # Capture a frame\n",
        "    if not ret:\n",
        "        print(\"Can't receive frame (stream end?). Exiting ...\")\n",
        "        break\n",
        "\n",
        "    # Preprocess the frame\n",
        "    resized_frame = cv2.resize(frame, (224, 224))\n",
        "    processed_frame = preprocess_image(resized_frame)  # Use the defined preprocessing function\n",
        "\n",
        "    # Generate caption\n",
        "    caption = generate_caption(processed_frame, model, idx2word, word2idx, max_caption_length)\n",
        "\n",
        "    # Display frame with caption\n",
        "    cv2.putText(frame, caption, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
        "    cv2.imshow('Live Captioning', frame)\n",
        "\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):  # Exit on pressing 'q'\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "WKEmyZCkkO-Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf359de9-25ea-4fdf-f92a-54a017423fd2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cannot open camera\n",
            "Starting camera. Press 'q' to exit.\n",
            "Can't receive frame (stream end?). Exiting ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "with open('model.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)"
      ],
      "metadata": {
        "id": "AzxW-UoNl-MN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}